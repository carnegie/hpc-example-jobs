{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5810984-12ae-4240-bf38-e796d2f1f200",
   "metadata": {},
   "source": [
    "# TensorBoard Scalars: Logging Metrics in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de407fd6-c30a-45c9-bf22-407307772a44",
   "metadata": {},
   "source": [
    "This example is a slightly motified tutorial from Tensorflow for using TensorBoard Scalars on Carnegie Clusters sourced from: https://www.tensorflow.org/tensorboard/scalars_and_keras\n",
    "\n",
    "In this example, Tensorflow uses TensorBoard's Time Series Dashboard in order to visualize key metrics for ML Training using an API (visualize default and custom scalars). This tutorial presents very basic examples to help you learn how to use these APIs with TensorBoard when developing your Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d317e2f6-8aff-4ce9-ba26-f810d2777b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.18.0'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import datetime, os\n",
    "# TensorBoard is provided through an Extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "from datetime import datetime\n",
    "from packaging import version\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "\n",
    "import numpy as np\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98dc4037-9a53-4a96-99a0-f306749dc528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/carnegie/nobackup/scratch/tkaminski/container_images/tensorboard_test/logs\n"
     ]
    }
   ],
   "source": [
    "# Define a log directory in your scratch or home folder, change this to whatever you want\n",
    "os.chdir('/carnegie/nobackup/scratch/tkaminski/container_images/tensorboard_test/logs')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e23c0eea-ce80-4c10-a7f1-166a9d22e7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = 1000\n",
    "# 80% of the data is for training.\n",
    "train_pct = 0.8\n",
    "\n",
    "train_size = int(data_size * train_pct)\n",
    "\n",
    "# Create some input data between -1 and 1 and randomize it.\n",
    "x = np.linspace(-1, 1, data_size)\n",
    "np.random.shuffle(x)\n",
    "\n",
    "# Generate the output data.\n",
    "# y = 0.5x + 2 + noise\n",
    "y = 0.5 * x + 2 + np.random.normal(0, 0.05, (data_size, ))\n",
    "\n",
    "# Split into test and train pairs.\n",
    "x_train, y_train = x[:train_size], y[:train_size]\n",
    "x_test, y_test = x[train_size:], y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7beea611-be45-47a3-9c33-2b0c2d996ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ... With default parameters, this takes less than 10 seconds.\n",
      "Average test loss:  0.05308009155560285\n"
     ]
    }
   ],
   "source": [
    "# Write the scalar values to TensorBoard logs\n",
    "logdir = \"scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(16, input_dim=1),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='mse', # keras.losses.mean_squared_error\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.2),\n",
    ")\n",
    "\n",
    "print(\"Training ... With default parameters, this takes less than 10 seconds.\")\n",
    "training_history = model.fit(\n",
    "    x_train, # input\n",
    "    y_train, # output\n",
    "    batch_size=train_size,\n",
    "    verbose=0, # Suppress chatty output; use Tensorboard instead\n",
    "    epochs=100,\n",
    "    validation_data=(x_test, y_test),\n",
    "    callbacks=[tensorboard_callback],\n",
    ")\n",
    "\n",
    "print(\"Average test loss: \", np.average(training_history.history['loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d45a73e-a87a-4e4e-81fc-3f378a60e5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch TensorBoard Dashboard with the log from the created log directory\n",
    "tensorboard --logdir=/carnegie/nobackup/scratch/tkaminski/container_images/tensorboard_test/logs/scalars --port=8888 --bind_all --path_prefix=/node/memex-2015-017.bsehpc.carnegiescience.edu/8888"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6a1960-1b78-403b-b26d-e8d63a582ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the scalar values to TensorBoard logs\n",
    "logdir = \"scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(logdir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "  \"\"\"\n",
    "  Returns a custom learning rate that decreases as epochs progress.\n",
    "  \"\"\"\n",
    "  learning_rate = 0.2\n",
    "  if epoch > 10:\n",
    "    learning_rate = 0.02\n",
    "  if epoch > 20:\n",
    "    learning_rate = 0.01\n",
    "  if epoch > 50:\n",
    "    learning_rate = 0.005\n",
    "\n",
    "  tf.summary.scalar('learning rate', data=learning_rate, step=epoch)\n",
    "  return learning_rate\n",
    "\n",
    "lr_callback = keras.callbacks.LearningRateScheduler(lr_schedule)\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(16, input_dim=1),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='mse', # keras.losses.mean_squared_error\n",
    "    optimizer=keras.optimizers.SGD(),\n",
    ")\n",
    "\n",
    "training_history = model.fit(\n",
    "    x_train, # input\n",
    "    y_train, # output\n",
    "    batch_size=train_size,\n",
    "    verbose=0, # Suppress chatty output; use Tensorboard instead\n",
    "    epochs=100,\n",
    "    validation_data=(x_test, y_test),\n",
    "    callbacks=[tensorboard_callback, lr_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7643aea5-ccf5-4c4e-8a96-0778ddb66024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "[[31.782534]\n",
      " [14.409847]\n",
      " [ 2.993511]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(x=np.array([60, 25, 2])))\n",
    "# True values to compare predictions against: \n",
    "# [[32.0]\n",
    "#  [14.5]\n",
    "#  [ 3.0]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
